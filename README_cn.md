# Transformer Copy Task with Attention Visualization

æœ¬é¡¹ç›®åŸºäºä¸€ç¯‡è®²è§£ Transformer åŸç†çš„æ–‡ç« åŠå…¶é™„å¸¦çš„ç¤ºä¾‹ä»£ç ï¼ˆå®ç°äº† Copy Task ä¸æ³¨æ„åŠ›å¯è§†åŒ–ï¼‰ï¼Œåœ¨å…¶åŸºç¡€ä¸Šè¿›è¡Œäº†**æ·±åº¦é‡æ„ä¸å¢å¼º**ã€‚æœ€ç»ˆå‘ˆç°çš„é¡¹ç›®ç»“æœä¸å¯è§†åŒ–æ•ˆæœå¦‚ä¸‹ï¼š

<img width="1607" height="885" alt="image" src="https://github.com/user-attachments/assets/432a00c6-6d73-4641-a82d-76c6c215f984" />


## ğŸ”§ é¡¹ç›®æ”¹è¿›å†…å®¹

1. **é€»è¾‘æ¢³ç†ä¸å‡½æ•°å°è£…**  
   å°†åŸå§‹ä»£ç ä¸­åˆ†æ•£ã€æ··ä¹±çš„é€»è¾‘è¿›è¡Œäº†ç³»ç»Ÿæ€§æ•´ç†ï¼Œå®Œæˆäº†å‡½æ•°åŒ–å’Œæ¨¡å—åŒ–ï¼Œä½¿æ•´ä½“æµç¨‹æ¸…æ™°æ˜“è¯»ï¼Œä¾¿äºå¤ç”¨ä¸æ‰©å±•ï¼š
   - `dataset.py`ï¼šç”Ÿæˆç”¨äº copy task çš„è®­ç»ƒä¸éªŒè¯æ•°æ®
   - `coderlayer_with_attn.py`ï¼šè‡ªå®šä¹‰ Encoder å’Œ Decoder å±‚ä»¥æå–çœŸå®æ³¨æ„åŠ›æƒé‡
   - `transformer.py`:åº”ç”¨è‡ªå®šä¹‰çš„Encoderå®ç°TransformerCopyModel
   - `train.py`ï¼šè®­ç»ƒä¸»é€»è¾‘
   - `visualize.py`ï¼šå¯è§†åŒ–å‡½æ•°
   - `main.py`ï¼šå®Œæ•´çš„è®­ç»ƒ + å¯è§†åŒ–æµç¨‹æ‰§è¡Œæ–‡ä»¶

3. **æ•°æ®ç”Ÿæˆæµç¨‹è§„èŒƒåŒ–**  
   ä¼˜åŒ–äº† Copy Task æ‰€ç”¨æ•°æ®é›†çš„ç”Ÿæˆæ–¹å¼ï¼šåŸç‰ˆä»£ç è®­ç»ƒé›†ä¸æµ‹è¯•é›†æ•°æ®ç”±åˆ†åˆ«è°ƒç”¨generate_dataå‡½æ•°ç”Ÿæˆï¼›æ›´æ–°åï¼Œè®­ç»ƒé›†ä¸æµ‹è¯•é›†ç”±å•æ¬¡è°ƒç”¨generate_dataå‡½æ•°ç”ŸæˆåŸå§‹æ•°æ®ï¼Œå†éšæœºåˆ†å‰²äº§ç”Ÿã€‚
   æ­¤å¤–ï¼Œè¿˜æ›´æ–°äº†è®­ç»ƒé›†ä¸æµ‹è¯•é›†çš„æ¯”ä¾‹ã€‚
   é€šè¿‡ä¼˜åŒ–ä½¿å…¶æ›´ç¬¦åˆç§‘å­¦ç ”ç©¶è§„èŒƒã€‚
    ```
    # before
    def generate_data(num_samples, seq_len, vocab_size):
        # éšæœºç”Ÿæˆæ•´æ•°åºåˆ—ï¼Œæ¯ä¸ªæ•´æ•°èŒƒå›´åœ¨ [1, vocab_size-1] å†…ï¼Œä¿ç•™ 0 ä½œä¸º padding çš„ä½ç½®
        data = np.random.randint(1, vocab_size, size=(num_samples, seq_len))
        return data
    # ç”Ÿæˆè®­ç»ƒå’Œæµ‹è¯•æ•°æ®
    train_data = generate_data(num_samples, seq_len, vocab_size)
    test_data = generate_data(200, seq_len, vocab_size)  # æµ‹è¯•æ ·æœ¬æ•°è¾ƒå°‘
    # å°† numpy æ•°ç»„è½¬æ¢ä¸º tensor
    train_input = torch.LongTensor(train_data)
    train_target = torch.LongTensor(train_data)  # å¤åˆ¶ä»»åŠ¡ï¼šç›®æ ‡ä¸è¾“å…¥ä¸€è‡´
    test_input = torch.LongTensor(test_data)
    test_target = torch.LongTensor(test_data)
    ```
    ```
    # after
    def generate_data(num_samples, seq_len, vocab_size):
        # éšæœºç”Ÿæˆæ•´æ•°åºåˆ—ï¼Œæ¯ä¸ªæ•´æ•°èŒƒå›´åœ¨ [1, vocab_size-1] å†…ï¼Œä¿ç•™ 0 ä½œä¸º padding çš„ä½ç½®
        data = np.random.randint(1, vocab_size, size=(num_samples, seq_len))
        return data
    def prepare_data(num_samples, seq_len, vocab_size, random_state, test_ratio=0.2):
        total_data = generate_data(num_samples, seq_len, vocab_size)
        train_data, test_data = train_test_split(total_data, test_size=test_ratio, random_state=random_state)
        train_input = torch.LongTensor(train_data)
        train_target = torch.LongTensor(train_data)
        test_input = torch.LongTensor(test_data)
        test_target = torch.LongTensor(test_data)
        return train_input, train_target, test_input, test_target
    ```
3. **å¯è§†åŒ–ç¾åŒ–ä¸ä¿®æ­£**  
   å¯¹åŸå§‹ attention å¯è§†åŒ–ä¸­å­˜åœ¨çš„ label é‡å é—®é¢˜è¿›è¡Œäº†è°ƒæ•´ä¸ä¿®å¤ï¼Œå›¾åƒæ’ç‰ˆæ›´åˆç†ã€æ ‡æ³¨æ›´æ¸…æ™°ã€‚
    ```
    # before: tight_layoutæ–¹æ³•
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    ```
    ```
    # after: constrained_layoutæ–¹æ³•
    plt.figure(figsize=(18, 12), constrained_layout=True)
    ```
    æ•ˆæœï¼šbefore(å·¦)â†’after(å³)
    <p align="center">
      <img src="https://github.com/user-attachments/assets/2051e636-0a24-4fb3-8209-ebb060af2b15" width="45%"/>
      <img src="https://github.com/user-attachments/assets/40b1678a-a7b0-41d5-8047-eb46639acdbc" width="45%"/>
    </p>

4. **çœŸå®æ³¨æ„åŠ›æƒé‡æå–ä¸å±•ç¤º**  
   æœ€é‡è¦çš„ä¸€ç‚¹ï¼šåŸå§‹é¡¹ç›®ä¸­å±•ç¤ºçš„æ³¨æ„åŠ›å›¾ä»…æ˜¯**è®­ç»ƒå¤–**è°ƒç”¨ `nn.MultiheadAttention` å±‚ç”Ÿæˆçš„æ¨¡æ‹Ÿæ•°æ®ï¼Œ**å¹¶éæ¨¡å‹å®é™… forward ä¸­çš„ attention weights**ã€‚  
   æœ¬é¡¹ç›®é€šè¿‡è‡ªå®šä¹‰ Transformer å±‚ï¼ˆcoderlayer_with_attn.py)ï¼ŒæˆåŠŸä»æ¨¡å‹å†…éƒ¨æå–çœŸæ­£çš„æ³¨æ„åŠ›çŸ©é˜µï¼Œå®ç°äº†æ›´å…·è§£é‡Šæ€§å’Œåˆ†æä»·å€¼çš„å¯è§†åŒ–ã€‚
   æ­¤å¤–ï¼Œé‰´äºæœ€ç»ˆç»“æœç»„å›¾ä¸­å…¶ä½™ä¸‰å¼ å›¾éƒ½æ˜¯æŒ‰ç…§ç¬›å¡å°”åæ ‡ç³»å‘ˆç°ï¼Œåœ¨æœ¬æ¬¡æ”¹è¿›ä¸­ï¼Œä¹Ÿå°†æ³¨æ„åŠ›å¯è§†åŒ–çƒ­å›¾ç”±åŸæœ¬çš„å›¾ç‰‡åæ ‡ç³»ï¼ˆåŸç‚¹åœ¨å·¦ä¸Šè§’ï¼Œxè½´ï¼škey(j), yè½´ï¼šquery(i)ï¼‰æ”¹ä¸ºç¬›å¡å°”åæ ‡ç³»ï¼ˆåŸç‚¹åœ¨å·¦ä¸‹è§’ï¼Œxè½´ï¼šquery(i), yè½´ï¼škey(j)ï¼‰å±•ç¤º,ä½¿å¾—query(i)ä¸key(j)ä¹‹é—´çš„æ³¨æ„åŠ›å…³ç³»æ›´åŠ ç›´è§‚ã€‚
    ```
    # before
    # ä¸ºäº†è·å–æ³¨æ„åŠ›æƒé‡ï¼Œæˆ‘ä»¬é‡æ–°æ„é€ ä¸€ä¸ªåŒ…å« MultiheadAttention çš„æ¨¡å—
    # æ³¨æ„ï¼šç”±äº nn.TransformerEncoderLayer å†…éƒ¨æ²¡æœ‰ç›´æ¥è¿”å›æ³¨æ„åŠ›æƒé‡ï¼Œæˆ‘ä»¬è¿™é‡Œå•ç‹¬ä½¿ç”¨ nn.MultiheadAttention æ¥æ¨¡æ‹Ÿå…¶ä¸­ä¸€å±‚æ³¨æ„åŠ›
    # é€‰å– sample_input çš„åµŒå…¥è¡¨ç¤ºä½œä¸ºæŸ¥è¯¢ã€é”®ã€å€¼ï¼Œè¦æ±‚ batch_first=True
    emb_sample = model.embedding(sample_input) * np.sqrt(model.d_model)
    emb_sample = model.pos_encoder(emb_sample)  # shape: (1, seq_len, d_model)
    multihead_attn = nn.MultiheadAttention(embed_dim=model.d_model, num_heads=8, batch_first=True)
    # é€šè¿‡ forward æ—¶è®¾ç½® need_weights=True å¾—åˆ°æ³¨æ„åŠ›æƒé‡
    attn_output, attn_weights = multihead_attn(emb_sample, emb_sample, emb_sample, need_weights=True, average_attn_weights=False)
    ```
    ```
    # after
    _ = model(sample_input)  # è§¦å‘ forwardï¼Œè®¡ç®—æ³¨æ„åŠ›æƒé‡
    attn_weights = model.last_attn  # è·å–æœ€åä¸€å±‚çš„æ³¨æ„åŠ›æƒé‡
    ```
    åæ ‡ç³»æ›´æ–°ï¼šbefore(å·¦)â†’after(å³)
    <p align="center">
      <img src="https://github.com/user-attachments/assets/2a9a1991-c066-43f1-8481-1ea2dadaa4c1" width="45%"/>
      <img src="https://github.com/user-attachments/assets/c5833b8c-0d4b-4155-b496-02fda99d11e1" width="45%"/>
    </p>
    æ³¨æ„åŠ›é€»è¾‘æ›´æ–°ï¼šbefore(å·¦)â†’after(å³)
    <p align="center">
      <img src="https://github.com/user-attachments/assets/be57481b-f7b6-48b5-bee2-c39420828e5a" width="45%"/>
      <img src="https://github.com/user-attachments/assets/629729a5-9c01-4b7d-a9ac-5cfbc4fa8188" width="45%"/>
    </p>

## ğŸ™ è‡´è°¢ä¸å¼•ç”¨

æœ¬é¡¹ç›®å‚è€ƒå¹¶åŸºäºå¾®ä¿¡å…¬ä¼—å·ã€Œæœºå™¨å­¦ä¹ åˆå­¦è€…ã€å‘å¸ƒçš„æ–‡ç«   
[ã€Šã€è®ºæ–‡å¤ç°ã€‘ä»é›¶å®ç°Transformerï¼Œå¹¶å¯è§†åŒ–Attentionï¼ã€‹](https://mp.weixin.qq.com/s/BCECx-0C9E_wY4ZyRrZ5uQ) ä¸­çš„æ•™å­¦ç¤ºä¾‹ä»£ç ï¼Œ  
åœ¨å…¶åŸºç¡€ä¸Šè¿›è¡Œäº†ç»“æ„é‡æ„ã€attention æƒé‡æå–æ–¹å¼ä¼˜åŒ–ã€å¯è§†åŒ–å¢å¼ºç­‰æ”¹è¿›ï¼Œè‡´è°¢åŸä½œè€…çš„åˆ†äº«ã€‚

## ğŸ“œ License

MIT License Â© 2025 PengTang
